<h2>Course Reflections</h2>
<h3>Mem 2 - Yashoda Talawar</br>Roll number: 357</h3>
<p>
  <strong>1.</strong> What are the kinds of problems we see in the nature? <br>
  <strong>Ans:</strong><br>
  <strong>Iteration:</strong> Repeating tasks, like summing a list of numbers, problems like calculating compound interest.<br>
  <strong>Recursion:</strong> Scenarios like Fibonacci sequence generation or solving the Tower of Hanoi involve recursive calls and factorial.<br>
  <strong>Backtracking:</strong> Exploring possibilities to find a solution, like solving a maze, n-queens, or generating all possible permutations using backtracking to explore solutions.
</p>

<p>
  <strong>2.</strong> What is space and time efficiency? Why are they important? Explain the different class of problems and orders of growth.<br>
  <strong>Ans:</strong><br>
  Time Efficiency measures the computational time required to execute an algorithm and represents the number of basic operations performed by an algorithm.<br>
  Space Efficiency measures the memory (storage) required by an algorithm, tracking how much memory is the input size and determining how much additional memory is needed to solve a problem.<br>
  A good algorithm executes quickly and saves space in the process. We should find a good medium of space and time (space and time complexity). In the world of computer science to perform better, we need to write algorithms that are time efficient and use less memory. It should be resource-optimized, solving larger problems with limited computing power and giving an impact on user experience and system performance.<br>
  <strong>Complexity Classes and Orders of Growth:</strong><br>
  <strong>Big O Notation (Worst-Case Complexity):</strong> Describes the upper bound of an algorithm's growth rate:<br>
  - <strong>O(1)</strong> - Constant Time: Fixed runtime regardless of input size (e.g., array access, simple arithmetic operations).<br>
  - <strong>O(log n)</strong> - Logarithmic Time: Runtime grows logarithmically (e.g., balanced tree operations).<br>
  - <strong>O(n)</strong> - Linear Time: Runtime grows linearly with input size (e.g., traversing an array).<br>
  - <strong>O(n log n)</strong> - Linearithmic Time: Efficient sorting algorithms like merge sort, heap sort.<br>
  - <strong>O(n²)</strong> - Quadratic Time: Nested iterations (e.g., bubble sort).<br>
  - <strong>O(2ⁿ)</strong> - Exponential Time: Runtime doubles with each input addition (e.g., recursive Fibonacci).<br>
  - <strong>O(n!)</strong> - Factorial Time: Extremely inefficient (e.g., traveling salesman problem brute force).<br>
  Visualization of Growth Rates: **O(1) < O(log n) < O(n) < O(n log n) < O(n²) < O(2ⁿ) < O(n!)**
</p>

<p>
  <strong>3.</strong> Take away from different design principles from chapter 2:<br>
  <strong>Modularity:</strong> Simplifies complex problems into smaller tasks.<br>
  <strong>Abstraction:</strong> Hides implementation details to focus on solving the problem.<br>
  <strong>Efficiency:</strong> Optimizing time and space complexities to improve performance.<br>
  <strong>Reusability:</strong> Algorithms like dynamic programming can be applied across multiple domains.
</p>
